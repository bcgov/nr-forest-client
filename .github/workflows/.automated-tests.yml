name: Automated user flow tests
run-name: Automated workflow tests triggered by @${{ github.actor }}

on:
  workflow_run:
    workflows: ["Pull Request Open"]
    types: [completed]

concurrency:
  group: ${{ github.event.workflow_run.pull_requests.number }}
  cancel-in-progress: true

jobs:
  vars:
    name: Variables
    runs-on: ubuntu-24.04
    outputs:
      url: ${{ steps.calculate.outputs.url }}
      event: ${{ steps.event_number.outputs.event }}
    steps:
      # steps.event_number.outputs.event => needs.vars.outputs.event
      - name: Calculate the event number
        id: event_number
        run: |
          echo "${{ toJSON(github.event.workflow_run) }}"
          echo "event=${GITHUB_REF#refs/pull/}" | sed 's|/.*||'" >> $GITHUB_OUTPUT
      # steps.calculate.outputs.url => needs.vars.outputs.url
      - name: Calculate the deployment number
        id: calculate
        run: |
          echo "url=${{ github.event.repository.name }}-$((${{ steps.event_number.outputs.event }} % 50))-frontend.apps.silver.devops.gov.bc.ca" >> $GITHUB_OUTPUT

  cypress-run:
    name: "User flow test"
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-24.04
    needs: [vars]
    environment: tools
    env:
      URL: ${{ needs.vars.outputs.url }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - uses: actions/setup-node@v4
        name: Start node
        with:
          node-version: 18

      - name: Run Cypress End-to-End
        uses: cypress-io/github-action@v5
        with:
          working-directory: cypress
        env:
          CYPRESS_baseUrl: https://${{ env.URL }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          CYPRESS_editor_password: ${{ secrets.UAT_EDITOR_PASSWORD }}
          CYPRESS_editor_username: ${{ secrets.UAT_EDITOR_USERNAME }}
          CYPRESS_admin_password: ${{ secrets.UAT_ADMIN_PASSWORD }}
          CYPRESS_admin_username: ${{ secrets.UAT_ADMIN_USERNAME }}
          CYPRESS_viewer_password: ${{ secrets.UAT_VIEWER_PASSWORD }}
          CYPRESS_viewer_username: ${{ secrets.UAT_VIEWER_USERNAME }}
          CYPRESS_bceid_password: ${{ secrets.UAT_BCEID_PASSWORD }}
          CYPRESS_bceid_username: ${{ secrets.UAT_BCEID_USERNAME }}
          CYPRESS_bcsc_password: ${{ secrets.UAT_BCSC_PASSWORD }}
          CYPRESS_bcsc_username: ${{ secrets.UAT_BCSC_USERNAME }}

      - name: Publish Cypress Results
        uses: mikepenz/action-junit-report@v5
        continue-on-error: true
        if: always()
        with:
          report_paths: cypress/result.xml
          commit: ${{ github.event.pull_request.head.sha }}
          summary: Cypress Test Results
          detailed_summary: true
          job_name: User Journeys

      - name: Check for Cypress Screenshots and Videos
        run: |
          if [ -d "cypress/cypress/screenshots" ] && [ "$(ls -A cypress/cypress/screenshots)" ]; then
            echo "Screenshots folder is not empty, uploading artifacts."            
            echo "screenshots=true" >> $GITHUB_OUTPUT

          else
            echo "Screenshots folder is empty or does not exist."
            echo "screenshots=false" >> $GITHUB_OUTPUT
          fi

          if [ -d "cypress/cypress/videos" ] && [ "$(ls -A cypress/cypress/videos)" ]; then
            echo "Videos folder is not empty, uploading artifacts."            
            echo "videos=true" >> $GITHUB_OUTPUT

          else
            echo "Videos folder is empty or does not exist."
            echo "videos=false" >> $GITHUB_OUTPUT
          fi
        id: check_artifacts

      - uses: actions/upload-artifact@v4
        name: Upload Cypress Screenshots
        if: always()
        with:
          name: cypress-screenshots
          path: cypress/cypress/screenshots
          retention-days: 7

      - uses: actions/upload-artifact@v4
        name: Upload Cypress Videos
        if: always()
        with:
          name: cypress-videos
          path: cypress/cypress/videos
          retention-days: 7

  scale-down-after:
    name: Scale down legacy
    needs: [cypress-run, vars]
    environment: dev
    if: always()    
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/checkout@v4
      - name: Install CLI tools from OpenShift Mirror
        uses: redhat-actions/openshift-tools-installer@v1
        with:        
          oc: "4.13"
      - name: Stop the Legacy Service
        continue-on-error: true
        run: |
          oc login --token=${{ secrets.OC_TOKEN }} --server=${{ secrets.OC_SERVER }}
          oc project ${{ secrets.OC_NAMESPACE }} # Safeguard!
          oc scale dc/nr-forest-client-${{ needs.vars.outputs.event }}-legacy --replicas=0
          undesired_replicas=0
          while true; do
            available_replicas=$(oc get dc/nr-forest-client-${{ needs.vars.outputs.event }}-legacy -n ${{ secrets.OC_NAMESPACE }} -o jsonpath='{.status.availableReplicas}')
            
            if [[ "$available_replicas" -ge "$undesired_replicas" ]]; then
              echo "DeploymentConfig ${{ secrets.OC_NAMESPACE }}-${{ needs.vars.outputs.event }}-legacy is now available with $available_replicas replicas."
              break
            fi
            
            echo "Waiting... ($available_replicas pods available)"
            sleep 5
          done

  recreate-database:
    name: Recreate database
    needs: [scale-down-after, vars]
    environment: tools
    if: always()    
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/checkout@v4
      - name: Install CLI tools from OpenShift Mirror
        uses: redhat-actions/openshift-tools-installer@v1
        with:        
          oc: "4.13"
      - name: Remove the PR database
        continue-on-error: true
        run: |
          oc login --token=${{ secrets.OC_TOKEN }} --server=${{ secrets.OC_SERVER }}
          oc project ${{ secrets.OC_NAMESPACE }} # Safeguard!
          # This removes a new pluggable database, user and service for the PR
          for i in {1..5}; do
            POD_NAME=$(oc get pods -l app=nr-forest-client-tools -l deployment=nr-forest-client-tools-legacydb -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
            if [ -n "$POD_NAME" ]; then
              echo "Pod found: $POD_NAME"
              oc exec $POD_NAME -- /opt/oracle/removeDatabase "THE" "PR_${{ needs.vars.outputs.event }}"
              break
            else
              echo "Pod not found, retrying in 10 seconds... ($i/5)"
              sleep 10
            fi
          done

          if [ -z "$POD_NAME" ]; then
            echo "Failed to find the pod after 5 attempts."
          fi

      - name: Create the PR database
        continue-on-error: true
        run: |
          oc login --token=${{ secrets.OC_TOKEN }} --server=${{ secrets.OC_SERVER }}
          oc project ${{ secrets.OC_NAMESPACE }} # Safeguard!
          # This creates a new pluggable database for the PR
          for i in {1..5}; do
            POD_NAME=$(oc get pods -l app=nr-forest-client-tools  -l deployment=nr-forest-client-tools-legacydb -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
            if [ -n "$POD_NAME" ]; then
              echo "Pod found: $POD_NAME"
              oc exec $POD_NAME -- /opt/oracle/createDatabase PR_${{ needs.vars.outputs.event }}
              break
            else
              echo "Pod not found, retrying in 10 seconds... ($i/5)"
              sleep 10
            fi
          done

          if [ -z "$POD_NAME" ]; then
            echo "Failed to find the pod after 5 attempts."
          fi

      - name: Create the PR user
        continue-on-error: true
        run: |
          oc login --token=${{ secrets.OC_TOKEN }} --server=${{ secrets.OC_SERVER }}
          oc project ${{ secrets.OC_NAMESPACE }} # Safeguard!
          # This creates a new pluggable database for the PR
          for i in {1..5}; do
            POD_NAME=$(oc get pods -l app=nr-forest-client-tools -l deployment=nr-forest-client-tools-legacydb -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
            if [ -n "$POD_NAME" ]; then
              echo "Pod found: $POD_NAME"
              oc exec $POD_NAME -- /opt/oracle/createAppUser "THE" "${{ secrets.ORACLEDB_PASSWORD_W }}_${{ needs.vars.outputs.event }}" "PR_${{ needs.vars.outputs.event }}"
              break
            else
              echo "Pod not found, retrying in 10 seconds... ($i/5)"
              sleep 10
            fi
          done

          if [ -z "$POD_NAME" ]; then
            echo "Failed to find the pod after 5 attempts."
          fi

      - name: Migrate the PR database
        continue-on-error: true
        run: |
          BRANCH_NAME="${{ github.head_ref }}"
          # Escape slashes and other special characters
          ESCAPED_BRANCH_NAME=$(echo "$BRANCH_NAME" | sed 's/[\/&]/\\&/g')
          oc login --token=${{ secrets.OC_TOKEN }} --server=${{ secrets.OC_SERVER }}
          oc project ${{ secrets.OC_NAMESPACE }} # Safeguard!
          oc create job --from=cronjob/nr-forest-client-tools-migratedb migrate-pr${{ needs.vars.outputs.event }}-${{ github.run_attempt }}-$(date +%s) --dry-run=client -o yaml | sed "s/value: main/value: ${ESCAPED_BRANCH_NAME}/" | sed "s/value: \"0\"/value: \"${{ needs.vars.outputs.event }}\"/" | oc apply -f -
  
  scale-up-legacy:
    name: Scale up legacy
    needs: [recreate-database, vars]
    environment: dev
    if: always()    
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/checkout@v4
      - name: Install CLI tools from OpenShift Mirror
        uses: redhat-actions/openshift-tools-installer@v1
        with:        
          oc: "4.13"
      - name: Start the Legacy Service
        continue-on-error: true
        run: |
          oc login --token=${{ secrets.OC_TOKEN }} --server=${{ secrets.OC_SERVER }}
          oc project ${{ secrets.OC_NAMESPACE }} # Safeguard!
          oc scale dc/nr-forest-client-${{ needs.vars.outputs.event }}-legacy --replicas=1

  results:
    name: PR Results
    needs: [cypress-run, scale-down-after, recreate-database, scale-up-legacy]
    if: always()
    runs-on: ubuntu-24.04
    steps:
      - if: contains(needs.*.result, 'failure')||contains(needs.*.result, 'canceled')
        run: echo "At least one job has failed." && exit 1
      - run: echo "Success!"
